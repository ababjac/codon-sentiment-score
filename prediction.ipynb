{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3285e0-48a2-4c09-aa72-cbd68fee43fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 04:30:46.105812: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-10 04:30:46.209114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 04:31:08.002301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import gc\n",
    "import helpers\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig, DistilBertPreTrainedModel, DistilBertModel\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55650d8-cbc1-41f1-bbce-669cddcef55a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PATH='/lustre/isaac/proj/UTK0196/codon-expression-data/fullTableForTrainning/'\n",
    "RUN=1\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "976c5a3d-e921-4254-85e2-451ecef7a054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(epred):\n",
    "    print('in compute_metrics')\n",
    "    # Computes metrics from specialized output from huggingface\n",
    "    #print(epred)\n",
    "\n",
    "    logits = epred[0]\n",
    "    labels = epred[1].reshape(-1,1)\n",
    "\n",
    "    print(logits.shape, labels.shape)\n",
    "    metrics = {}\n",
    "\n",
    "    metrics['mse'] = mean_squared_error(labels, logits)\n",
    "    metrics['mae'] = mean_absolute_error(labels, logits)\n",
    "    metrics['r2'] = r2_score(labels, logits)\n",
    "    rho, pval = spearmanr(labels, logits)\n",
    "    metrics['spearmanr'] = rho\n",
    "    #metrics['single_squared_errors'] = ((logits - labels).flatten()**2).tolist()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9835e814-08de-4e4f-bbd4-76dc4df0c26f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        print('in compute_loss')\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        #print(outputs)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "\n",
    "        print(labels.shape, logits.shape)\n",
    "\n",
    "        # compute custom loss\n",
    "        loss_fct = torch.nn.MSELoss()\n",
    "        loss = loss_fct(labels.float(), logits)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "#https://stackoverflow.com/questions/67689219/copy-one-layers-weights-from-one-huggingface-bert-model-to-another\n",
    "def setLayers(t, s, parts):\n",
    "    target = dict(t.named_parameters())\n",
    "    source = dict(s.named_parameters())\n",
    "\n",
    "    #print(any('bert.embeddings.word_embeddings.weight' for val in source.keys()))\n",
    "\n",
    "    for part in parts:\n",
    "        target[part].data.copy_(source[part].data)  \n",
    "        #target[part].requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29a170de-bc95-4845-8491-b5daaa04fa5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parts = [\n",
    "        'bert.embeddings.word_embeddings.weight',\n",
    "        'bert.embeddings.position_embeddings.weight',\n",
    "        'bert.embeddings.token_type_embeddings.weight',\n",
    "        'bert.embeddings.LayerNorm.weight',\n",
    "        'bert.embeddings.LayerNorm.bias',\n",
    "        'bert.encoder.layer.0.attention.self.query.weight',\n",
    "        'bert.encoder.layer.0.attention.self.query.bias',\n",
    "        'bert.encoder.layer.0.attention.self.key.weight',\n",
    "        'bert.encoder.layer.0.attention.self.key.bias',\n",
    "        'bert.encoder.layer.0.attention.self.value.weight',\n",
    "        'bert.encoder.layer.0.attention.self.value.bias',\n",
    "        'bert.encoder.layer.0.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.0.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.0.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.0.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.0.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.0.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.0.output.dense.weight',\n",
    "        'bert.encoder.layer.0.output.dense.bias',\n",
    "        'bert.encoder.layer.0.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.0.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.1.attention.self.query.weight',\n",
    "        'bert.encoder.layer.1.attention.self.query.bias',\n",
    "        'bert.encoder.layer.1.attention.self.key.weight',\n",
    "        'bert.encoder.layer.1.attention.self.key.bias',\n",
    "        'bert.encoder.layer.1.attention.self.value.weight',\n",
    "        'bert.encoder.layer.1.attention.self.value.bias',\n",
    "        'bert.encoder.layer.1.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.1.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.1.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.1.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.1.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.1.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.1.output.dense.weight',\n",
    "        'bert.encoder.layer.1.output.dense.bias',\n",
    "        'bert.encoder.layer.1.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.1.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.2.attention.self.query.weight',\n",
    "        'bert.encoder.layer.2.attention.self.query.bias',\n",
    "        'bert.encoder.layer.2.attention.self.key.weight',\n",
    "        'bert.encoder.layer.2.attention.self.key.bias',\n",
    "        'bert.encoder.layer.2.attention.self.value.weight',\n",
    "        'bert.encoder.layer.2.attention.self.value.bias',\n",
    "        'bert.encoder.layer.2.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.2.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.2.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.2.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.2.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.2.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.2.output.dense.weight',\n",
    "        'bert.encoder.layer.2.output.dense.bias',\n",
    "        'bert.encoder.layer.2.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.2.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.3.attention.self.query.weight',\n",
    "        'bert.encoder.layer.3.attention.self.query.bias',\n",
    "        'bert.encoder.layer.3.attention.self.key.weight',\n",
    "        'bert.encoder.layer.3.attention.self.key.bias',\n",
    "        'bert.encoder.layer.3.attention.self.value.weight',\n",
    "        'bert.encoder.layer.3.attention.self.value.bias',\n",
    "        'bert.encoder.layer.3.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.3.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.3.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.3.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.3.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.3.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.3.output.dense.weight',\n",
    "        'bert.encoder.layer.3.output.dense.bias',\n",
    "        'bert.encoder.layer.3.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.3.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.4.attention.self.query.weight',\n",
    "        'bert.encoder.layer.4.attention.self.query.bias',\n",
    "        'bert.encoder.layer.4.attention.self.key.weight',\n",
    "        'bert.encoder.layer.4.attention.self.key.bias',\n",
    "        'bert.encoder.layer.4.attention.self.value.weight',\n",
    "        'bert.encoder.layer.4.attention.self.value.bias',\n",
    "        'bert.encoder.layer.4.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.4.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.4.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.4.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.4.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.4.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.4.output.dense.weight',\n",
    "        'bert.encoder.layer.4.output.dense.bias',\n",
    "        'bert.encoder.layer.4.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.4.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.5.attention.self.query.weight',\n",
    "        'bert.encoder.layer.5.attention.self.query.bias',\n",
    "        'bert.encoder.layer.5.attention.self.key.weight',\n",
    "        'bert.encoder.layer.5.attention.self.key.bias',\n",
    "        'bert.encoder.layer.5.attention.self.value.weight',\n",
    "        'bert.encoder.layer.5.attention.self.value.bias',\n",
    "        'bert.encoder.layer.5.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.5.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.5.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.5.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.5.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.5.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.5.output.dense.weight',\n",
    "        'bert.encoder.layer.5.output.dense.bias',\n",
    "        'bert.encoder.layer.5.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.5.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.6.attention.self.query.weight',\n",
    "        'bert.encoder.layer.6.attention.self.query.bias',\n",
    "        'bert.encoder.layer.6.attention.self.key.weight',\n",
    "        'bert.encoder.layer.6.attention.self.key.bias',\n",
    "        'bert.encoder.layer.6.attention.self.value.weight',\n",
    "        'bert.encoder.layer.6.attention.self.value.bias',\n",
    "        'bert.encoder.layer.6.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.6.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.6.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.6.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.6.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.6.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.6.output.dense.weight',\n",
    "        'bert.encoder.layer.6.output.dense.bias',\n",
    "        'bert.encoder.layer.6.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.6.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.7.attention.self.query.weight',\n",
    "        'bert.encoder.layer.7.attention.self.query.bias',\n",
    "        'bert.encoder.layer.7.attention.self.key.weight',\n",
    "        'bert.encoder.layer.7.attention.self.key.bias',\n",
    "        'bert.encoder.layer.7.attention.self.value.weight',\n",
    "        'bert.encoder.layer.7.attention.self.value.bias',\n",
    "        'bert.encoder.layer.7.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.7.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.7.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.7.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.7.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.7.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.7.output.dense.weight',\n",
    "        'bert.encoder.layer.7.output.dense.bias',\n",
    "        'bert.encoder.layer.7.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.7.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.8.attention.self.query.weight',\n",
    "        'bert.encoder.layer.8.attention.self.query.bias',\n",
    "        'bert.encoder.layer.8.attention.self.key.weight',\n",
    "        'bert.encoder.layer.8.attention.self.key.bias',\n",
    "        'bert.encoder.layer.8.attention.self.value.weight',\n",
    "        'bert.encoder.layer.8.attention.self.value.bias',\n",
    "        'bert.encoder.layer.8.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.8.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.8.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.8.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.8.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.8.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.8.output.dense.weight',\n",
    "        'bert.encoder.layer.8.output.dense.bias',\n",
    "        'bert.encoder.layer.8.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.8.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.9.attention.self.query.weight',\n",
    "        'bert.encoder.layer.9.attention.self.query.bias',\n",
    "        'bert.encoder.layer.9.attention.self.key.weight',\n",
    "        'bert.encoder.layer.9.attention.self.key.bias',\n",
    "        'bert.encoder.layer.9.attention.self.value.weight',\n",
    "        'bert.encoder.layer.9.attention.self.value.bias',\n",
    "        'bert.encoder.layer.9.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.9.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.9.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.9.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.9.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.9.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.9.output.dense.weight',\n",
    "        'bert.encoder.layer.9.output.dense.bias',\n",
    "        'bert.encoder.layer.9.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.9.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.10.attention.self.query.weight',\n",
    "        'bert.encoder.layer.10.attention.self.query.bias',\n",
    "        'bert.encoder.layer.10.attention.self.key.weight',\n",
    "        'bert.encoder.layer.10.attention.self.key.bias',\n",
    "        'bert.encoder.layer.10.attention.self.value.weight',\n",
    "        'bert.encoder.layer.10.attention.self.value.bias',\n",
    "        'bert.encoder.layer.10.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.10.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.10.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.10.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.10.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.10.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.10.output.dense.weight',\n",
    "        'bert.encoder.layer.10.output.dense.bias',\n",
    "        'bert.encoder.layer.10.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.10.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.11.attention.self.query.weight',\n",
    "        'bert.encoder.layer.11.attention.self.query.bias',\n",
    "        'bert.encoder.layer.11.attention.self.key.weight',\n",
    "        'bert.encoder.layer.11.attention.self.key.bias',\n",
    "        'bert.encoder.layer.11.attention.self.value.weight',\n",
    "        'bert.encoder.layer.11.attention.self.value.bias',\n",
    "        'bert.encoder.layer.11.attention.output.dense.weight',\n",
    "        'bert.encoder.layer.11.attention.output.dense.bias',\n",
    "        'bert.encoder.layer.11.attention.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.11.attention.output.LayerNorm.bias',\n",
    "        'bert.encoder.layer.11.intermediate.dense.weight',\n",
    "        'bert.encoder.layer.11.intermediate.dense.bias',\n",
    "        'bert.encoder.layer.11.output.dense.weight',\n",
    "        'bert.encoder.layer.11.output.dense.bias',\n",
    "        'bert.encoder.layer.11.output.LayerNorm.weight',\n",
    "        'bert.encoder.layer.11.output.LayerNorm.bias',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efa15df4-cb6a-49d1-9b70-5c13214a3cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Tells the model we need to use the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb98679e-c892-4fba-9d37-be01afaf80a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/isaac/scratch/ababjac/codon-sentiment-score/helpers.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['codons_cleaned'] = get_codon_list(df[col])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12330 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1371 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3426 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Reading data...')\n",
    "filelist = os.listdir(PATH) \n",
    "df_list = [pd.read_csv(PATH+file) for file in filelist]\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "s = []\n",
    "for file, _df in zip(filelist, df_list):\n",
    "    species = file.partition('_')[0]\n",
    "    l = [species]*len(_df)\n",
    "    s.extend(l)\n",
    "    \n",
    "df['species'] = s\n",
    "\n",
    "SPECIES = 'Musmusculus'\n",
    "\n",
    "df = df[df['species'] == SPECIES] #train on only yeast sequences\n",
    "\n",
    "df = helpers.add_codons_to_df(df, 'Sequence')\n",
    "labels = normalize([np.log(df['median_exp'])])[0]\n",
    "\n",
    "#print(labels)\n",
    "\n",
    "#labels = labels.type(torch.LongTensor)\n",
    "\n",
    "classification_df = pd.DataFrame({'text' : df['codons_cleaned'], 'label' : labels})\n",
    "#MAX = int(max([(len(elem) / 3) for elem in df['codons_cleaned']])) #get max sequence length for padding\n",
    "#MED = int(np.median([(len(elem) / 3) for elem in df['codons_cleaned']])) #get median sequence length for padding\n",
    "#print(MED)\n",
    "#trunc_len = int((MAX + MED) / 2) #set truncation somewhere between max and median\n",
    "trunc_len = 1064\n",
    "\n",
    "df_train, df_test = train_test_split(classification_df, test_size=0.2, random_state=1234)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.1, random_state=1234)\n",
    "\n",
    "#print(len(df_val))\n",
    "\n",
    "ds_train = Dataset.from_pandas(df_train)\n",
    "ds_val = Dataset.from_pandas(df_val)\n",
    "ds_test = Dataset.from_pandas(df_test)\n",
    "\n",
    "del classification_df\n",
    "del df_train\n",
    "del df_test\n",
    "del df_val\n",
    "\n",
    "print('Tokenizing...')\n",
    "config = AutoConfig.from_pretrained('bert-base-uncased', max_position_embeddings=trunc_len, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained('./tokenizers/codonBERT', model_max_length=trunc_len, padding_side='left', truncation_side='right')\n",
    "\n",
    "\n",
    "tokenized_ds_train = ds_train.map(lambda d : tokenizer(d['text'], truncation=True, padding=True), batched=True)\n",
    "tokenized_ds_val = ds_val.map(lambda d : tokenizer(d['text'], truncation=True, padding=True), batched=True)\n",
    "tokenized_ds_test = ds_test.map(lambda d : tokenizer(d['text'], truncation=True, padding=True), batched=True)\n",
    "del ds_train\n",
    "del ds_val\n",
    "del ds_test\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a00f9f-0445-478d-ae9f-e8ea7c54f34e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model...\n"
     ]
    }
   ],
   "source": [
    "print('Building Model...')\n",
    "pretrained_model = AutoModelForSequenceClassification.from_pretrained('./models/codonBERT_binary_reg_celegan-pre-norm2/checkpoint-74087/')\n",
    "#model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "#setLayers(model, pretrained_model, parts) #setting weights from pretrained binary classifier except for last layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f958b09-a2a7-43ed-b640-45422e5082ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./models/codonBERT_binary_reg_{}-pre-norm2-1'.format(SPECIES),\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"spearmanr\",\n",
    "    load_best_model_at_end=True,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds_train,\n",
    "    eval_dataset=tokenized_ds_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adb5e3d-c3c5-44bf-ab37-9f1e55bc6f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([8, 1])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='429' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 67/429 41:53 < 3:49:44, 0.03 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n",
      "torch.Size([8]) torch.Size([8, 1])\n",
      "in compute_loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_ds_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m logits, labels, metrics \u001b[38;5;241m=\u001b[39m out\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/trainer.py:3069\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3066\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3068\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3069\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3072\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/trainer.py:3174\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3171\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[1;32m   3173\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[0;32m-> 3174\u001b[0m loss, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3175\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_inputs_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/trainer.py:3429\u001b[0m, in \u001b[0;36mTrainer.prediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_labels \u001b[38;5;129;01mor\u001b[39;00m loss_without_labels:\n\u001b[1;32m   3428\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3429\u001b[0m         loss, outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3430\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m   3432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m):\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mCustomTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1013\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1014\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1015\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[0;32m-> 1020\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1033\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    608\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 610\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    534\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    535\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 537\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/pytorch_utils.py:236\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    549\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 550\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/lustre/isaac/scratch/ababjac/pyvenv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out = trainer.predict(test_dataset=tokenized_ds_test)\n",
    "logits, labels, metrics = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e82c2-4725-4efa-a8cb-f2a2a94f3399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./results/codonBERT_binary_reg_scores_{}-pre-norm2.txt'.format(SPECIES),'w') as data: \n",
    "    data.write(str(metrics))\n",
    "\n",
    "with open('./results/codonBERT_binary_reg_output_{}-pre-norm2.txt'.format(SPECIES),'w') as data:\n",
    "    for val in logits:\n",
    "        data.write(str(val)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
